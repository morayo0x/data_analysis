{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---|||\n",
    "# Pandas  Introduction \n",
    "\n",
    "It is a Python library used for data manipulation, cleaning and processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The core data structure in pandas are Series (column like), and Dataframe(tabular like)\n",
    "---|||\n",
    "\n",
    "**Series**: one dimensional array-like object containing\n",
    "-  sequence of values, **P**\n",
    "-  an associated array of data labels, called its **index**\n",
    "\n",
    "> By default, the index ranges from 0 to the len(P) - 1 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notice the associated indices\n",
    "pd.Series([5, 10, 111, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a series with modified index, using its index parameter\n",
    "\n",
    "s = pd.Series([10, 20, 30, 40], index=['i', 'j', 'k', 'l'])\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain the index of a given series using the index attribute\n",
    "\n",
    "s.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---|||\n",
    "##### The index of a Series can be used to select data corresponding to the index\n",
    "\n",
    "They act as index to the data sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s['k'], s['i']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### NumPy-like operations can be used to manipulate a Series object\n",
    "\n",
    "The index remains unchanged, after the operation(s) is performed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s[s > 25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.exp(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query if the series contains a given index\n",
    "\n",
    "'j' in s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A good mental model is to think of a Series object as a dictionary of keys and values\n",
    "\n",
    "- the index are the keys\n",
    "- the data are the values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some states and their corresponding capitals in Nigeria\n",
    "sdata = {'lagos': 'ikeja', 'ogun': 'abeokuta', 'adamawa': 'lafia'}\n",
    "\n",
    "# create a series object from the data\n",
    "s = pd.Series(sdata)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the index\n",
    "\n",
    "s.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query if an index is contained in the series\n",
    "\n",
    "'lagos' in s, 'fct' in s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---|||\n",
    "\n",
    "#### Alter the indices of a Series in-place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = ['l', 'o', 'a']\n",
    "\n",
    "# change the indices to index\n",
    "s.index = index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---|||\n",
    "#### Using the Dictionary to create the series will sort the data based on the keys\n",
    "\n",
    "This can be overriden by passing the same keys, and in whatever order to the index keyword\n",
    "\n",
    "> adding a key that doesn't belong in the dictionary will result in the key having a NAN value (a.k.a missing data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding more indices than data, results in NAN values\n",
    "\n",
    "# some states and their corresponding capitals in Nigeria\n",
    "sdata = {'lagos': 'ikeja', 'ogun': 'abeokuta', 'adamawa': 'lafia'}\n",
    "\n",
    "# using the keys to order the values\n",
    "s = pd.Series(sdata, index=['lagos', 'adamawa', 'ogun'])\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using a key that is not in the dictionary => 'fct\n",
    "# will add the key, but its value will benan\n",
    "\n",
    "\n",
    "s = pd.Series(sdata, index=['ogun', 'fct', 'adamawa', 'lagos'])\n",
    "\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---|||\n",
    "#### **isnull** and **notnull** method as a way of detecting missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find index with missing data in a series\n",
    "\n",
    "s.isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find index without missing data in the series\n",
    "\n",
    "s.notnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the query formats (functions and instance method) are equivalent\n",
    "\n",
    "\n",
    "(\n",
    "    pd.notnull(s) == s.notnull(),\n",
    "    \n",
    "    pd.isnull(s) == s.isnull()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---|||\n",
    "##### Arithmetic Operations on series with similar keys \n",
    "\n",
    "When performing arithmetic operations on different series with similar keys, the keys are used to align the data before the operation is performed element wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata = {'i': 11, 'j': 33, 'k': 23}\n",
    "bdata = {'a': 100, 'j': 23, 'b': 73, 'k': 1000} # keys j, k are in adata\n",
    "\n",
    "s1 = pd.Series(adata)\n",
    "s2 = pd.Series(bdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notice that not all the keys are the same\n",
    "\n",
    "# additional keys thar are not present in either index(es) ...\n",
    "# wi;; return nan values\n",
    "\n",
    "s1 + s2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|^|\n",
    "\n",
    "Notice that keys that doesn't match have NAN values returned\n",
    "\n",
    "---\n",
    "---|||\n",
    "#### Naming a Series \n",
    "\n",
    "It is possible to name a series object using the name attribute of kwarg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pd.Series(sdata, name='States and Capital in Nigeria')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the name using its index attributes\n",
    "\n",
    "s.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---|||\n",
    "# Dataframe\n",
    "\n",
    "The other core pandas object is the DataFrame representing a tabular-like data (like excel spreadsheet). It contains\n",
    "\n",
    "- ordered collections of columns; each column can contain different data type\n",
    "- a row and column index\n",
    " \n",
    "A good mental model is to think of Dataframe as a dictionary containing\n",
    "\n",
    "- **keys**: representing the columns, and its column index\n",
    "-\n",
    "- **value**:  Series object such that\n",
    "  -  the keys of the series represent the row index\n",
    "  - the values in the series represent the data keyed by its row and column index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a dataframe\n",
    "\n",
    "data = {'states': ['lagos', 'fct', 'ondo', 'oyo', 'plateau'], 'capital': ['ikeja', 'abuja', 'akure', 'ibadan', 'jos']}\n",
    "\n",
    "# the row index will automatically default to a number, \n",
    "# if we had used the dictionary as is.\n",
    "# but we pass one in the intialisation\n",
    "\n",
    "df = pd.DataFrame(data, index=['i', 'j', 'k', 'l', 'm'])\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe by passing list\n",
    "# and we can specify the column index...\n",
    "# in the order we want them to appear\n",
    "\n",
    "df = pd.DataFrame(\n",
    "  [('lagos', 'ikeja', 'SW'),\n",
    "   ('fct', 'abuja', 'NC'), \n",
    "   ('ondo', 'akure', 'SW'),\n",
    "   ('oyo', 'ibadan', 'SW'),\n",
    "   ('plateau', 'jos', 'SS')],\n",
    "  columns=['states', 'capitals', 'geographic_region'], index=['a', 'b', 'c', 'd', 'e']\n",
    ")\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we passed this directly, then the columns will be ... \n",
    "# ordered based on the keys - (capital, geographic_region, states)\n",
    "\n",
    "data = {\n",
    "    'states': ['lagos', 'fct', 'ondo', 'oyo', 'plateau'],\n",
    "    'capital': ['ikeja', 'abuja', 'akure', 'ibadan', 'jos'],\n",
    "    'geographic_region': ['SW', 'NC', 'SW', 'SW', 'SS']\n",
    "    }\n",
    "\n",
    "# notice the order of the columns indices (geographic.., states, capital)\n",
    "\n",
    "# also notice that the population index in columns ... \n",
    "# is not contained in the data, so its values will be nan\n",
    "\n",
    "df = pd.DataFrame(data, columns=['geographic_region', 'states', 'capital', 'population'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---|||\n",
    "#### **Head** or **Tail** Selecting the top or last few elements\n",
    "\n",
    "We can select the first few elements in the beginning or end of the dataframe using the **head** and **tail** method.\n",
    "\n",
    "By default they return the first or last five rows in the dataframe, however, we can pass-in a number to indicate the number of rows that should be returned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the first n values of a dataframe\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the column index using the column attributes\n",
    "\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---|||\n",
    "\n",
    "# Indexing a Dataframe\n",
    "\n",
    "Remember that the keys to  the dataframe are its columns index.\n",
    "\n",
    "When the dataframe object is indexed by the column name, a series containing the row index and its corresponding data is returned.\n",
    "\n",
    "Indexing can be carried out in two ways; \n",
    "- **dictionary indexing**: as key \n",
    "- **attribute indexing**: using the name of the column\n",
    "\n",
    "> the dictionary indexing format is more general, as it can also be used with column index with *space* in their name, which would have other_wise be invalid using attribute. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the data in the capital -> a series \n",
    "\n",
    "df['capital']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using attribute index\n",
    "\n",
    "df.geographic_region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the population column\n",
    "\n",
    "df.population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign values to the population column\n",
    "\n",
    "df.population = [20, 10, 3, 4, 1]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remember broadcasting?\n",
    "\n",
    "df.population = 1\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---|||\n",
    "\n",
    "#### Adding a new series to an existing dataframe object\n",
    "\n",
    "Add a new column then;  add a series to a new column in the dataframe\n",
    "\n",
    "Satisfy the following\n",
    "\n",
    "- The length of the series data must match the those in the dataframe\n",
    "- The index length of the series must match those in the dataframe, \n",
    "- the index names,that matches those in the dataframe will be aligned\n",
    "- the index name that doesn't match will be NAN\n",
    "\n",
    "\n",
    "> Note, if the index length, those of the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the row index names\n",
    "\n",
    "df.index = ['one', 'two', 'three', 'four', 'five']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a series\n",
    "\n",
    "\n",
    "# the max length of the series must match those of the dataframe\n",
    "\n",
    "# the row index, 'five' is not on the new series index ... \n",
    "# hence the corresponding value will be nan\n",
    "\n",
    "# there is no 'not-good' index in the data frame, ... \n",
    "# hence its values will not be aligned\n",
    "\n",
    "s = pd.Series(\n",
    "    data=['no', 'yes', 'yes', 'yes', 'bad'], \n",
    "    index=['two', 'one', 'four', 'three', 'not-good']\n",
    "    )\n",
    "\n",
    "# add a new column to the dataframe\n",
    "# note this can only be created using dictionary key indexing\n",
    "# as using attribute indexing will not work\n",
    "\n",
    "df['Visited'] = np.nan\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the series to the visited column\n",
    "\n",
    "# notice that the 'not-good' column doesn't match\n",
    "\n",
    "df.Visited = s\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---|||\n",
    "#### Deleting a column from a dataframe\n",
    "\n",
    "Using the **del** keyword followed by the column selection from the dataframe will delete the column from the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df['Visited']\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---||| \n",
    "\n",
    "#### Swapping Columns and Rows with transpose\n",
    "\n",
    "Using the **transpose function** or **T attribute** will:\n",
    "\n",
    "- swap rows to columns\n",
    "- columns to rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---|||\n",
    "\n",
    "### Creating a new Dataframe from an existing Dataframe\n",
    "\n",
    "By using an existing dataframe object, one can create a new dataframe.\n",
    "\n",
    "IF the index key is also specified, then\n",
    "\n",
    "- any index of the previous dataframe added to this new index, will have its column data in the new data frame\n",
    "\n",
    "- new index will automatically be assigned nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we only want the data in 'one, five and three' in the new dataframe\n",
    "\n",
    "# in the new index added, 'ten, nine' will be assigned nan \n",
    "\n",
    "df2 = pd.DataFrame(df, index=['one', 'ten', 'five', 'nine', 'three'])\n",
    "\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the name attributes of the index and column of a dataframe\n",
    "\n",
    "df2.columns.name = 'States in Nigeria Info'\n",
    "df2.index.name = \"Numbering\"\n",
    "\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return the values in a dataframe\n",
    "# the data are returned along the columns axis\n",
    "\n",
    "df.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---|||\n",
    "\n",
    "### Index Object\n",
    "\n",
    "This is a pandas object that holds the values of\n",
    "- a Series row index \n",
    "- a Dataframe's columns or row index A\n",
    "\n",
    "The Index Obeject is;\n",
    "\n",
    "- It is immutable \n",
    "- it can be shared among other data structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manipulatiing a Series using its index object\n",
    "\n",
    "s = pd.Series(\n",
    "    data=['Mo', 'Usman', 'Kolawole'],\n",
    "    index=['a', 'b', 'c']\n",
    "    )\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the index object\n",
    "\n",
    "ind = s.index\n",
    "\n",
    "ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index object are immutable\n",
    "\n",
    "ind[0] = 'k'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  create an index object\n",
    "\n",
    "ind = pd.Index(data=['l', 'm', 'n', 'p'])\n",
    "\n",
    "ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the new index object in a series\n",
    "\n",
    "s = pd.Series(np.arange(4), index=ind)\n",
    "\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind2 = ['l', 'm', 'n', 'p']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remember the difference between (==) and (is)?\n",
    "\n",
    "# compares element wise; remember vectorization?\n",
    "s.index == ind2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that they are the same object in memory \n",
    "\n",
    "s.index is ind2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.index is ind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---|||\n",
    "### Index Object as a container for Duplicate Object\n",
    "\n",
    "Since Index Object are immutable, they are similar to a fixed-set in Python, and support Set logic. But unlike Python Sets, they can contain duplicate values   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create two pandas object\n",
    "\n",
    "ind1 = pd.Index(['a', 'b', 'c', 'd', 'm', 'f', 'g', 'l'],)\n",
    "ind2 = pd.Index(['l', 'm', 'b', 'p', 'a', 'h', 'q', 'r'],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply set logic to both\n",
    "\n",
    "# concatenate two Index objects to create a new one\n",
    "\n",
    "ind3 = ind2.append(ind1)\n",
    "\n",
    "ind3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the difference between two set A - B\n",
    "# this is remove all the elements of B that is also in A,  from A\n",
    "\n",
    "ind4 = ind3.difference(ind2)\n",
    "\n",
    "ind4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the union of two or more\n",
    "\n",
    "# remember this is set logic, so duplicates will not be allowed\n",
    "# but an index object itself can contain duplicate values\n",
    "\n",
    "ind5 = ind1.union(ind2)\n",
    "\n",
    "ind5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---|||\n",
    "### Re-indexing a Series \n",
    "\n",
    "The **reindex** method allows creating a new Series object, with the data of the old series **aligned** to a new index (if the index of the data in the old series are elements in the new index).\n",
    "\n",
    "> the data in the new index will be ordered based on how they are passed in the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a series\n",
    "\n",
    "s = pd.Series(\n",
    "    data=np.arange(8),\n",
    "    index=['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h']\n",
    "    )\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index\n",
    "\n",
    "s.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the index using the reindex method\n",
    "\n",
    "# note there are new index values present, \n",
    "# so, index in **s** will be aligned to its data in the new series ...\n",
    "# and new index values will be assigned NAN values\n",
    "\n",
    "s.reindex(['a', 'i', 'j', 'k', 'b', 'l', 'm', 'c', 'n', 'p', 'd', 'q'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---|||\n",
    "### Filling the NAN values in the **reindex** method\n",
    "\n",
    "When both old and new index values are passed as elements in the reindex method,\n",
    "the new index values will by default be **aligned** to NAN values. To fill this NAN values, one can pass the method of filling the NAN values.\n",
    "\n",
    "**Methods**\n",
    "\n",
    "- ffill: fill the NAN values with the last valid data before it,\n",
    "- bfill: fill the NAN with the first valid data that comes after it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the same series, from above\n",
    "\n",
    "s.reindex(\n",
    "    index=['a', 'i' , 'b', 'j', 'c', 'k', 'd', 'e'],\n",
    "    method='ffill'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.reindex?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---|||\n",
    "### NumPy UFunc and Mappings \n",
    "\n",
    "Remember UFuncs are numpy functions that apply an operation to each element of an ndarray, through broadcasting.\n",
    "\n",
    "In pandas, ufuncs can also be applied to pandas dataframe and series objects\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.arange(24).reshape((6 , 4)))\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the sqrt ufunc of the table\n",
    "\n",
    "np.sqrt(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---|||\n",
    "### Apply Mapping Method\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pd.Series(np.arange(1, 10))\n",
    "df = pd.DataFrame(np.arange(36).reshape(9, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f =  lambda x: x*x\n",
    "\n",
    "def g(x):\n",
    "    return pd.Series([x.min(), x.max()], index=['min', 'max'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.apply(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.apply(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.apply(g, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---|||\n",
    "### **applymap** method\n",
    "\n",
    "This is similar to apply, except that it applies the passed in function to each element in the dataframe, instead of applying it to a a series along a  specific axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.random.random((5, 6))\n",
    "df = pd.DataFrame(data, index=['a', 'b', 'c', 'd', 'e'])\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "format = lambda x: '%.3f' %x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using apply will try to execute format on the axis (0, by default),\n",
    "# which should fail, because the format method except single values,\n",
    "# but the apply method will pass a series object to it\n",
    "\n",
    "df.apply(format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to overcome this challenge, the applymap method is used\n",
    "\n",
    "df.applymap(format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---|||\n",
    "### Sorting\n",
    "\n",
    "Sorting can be done in two ways:\n",
    "\n",
    "- **by values**: sort based on the value in the series or dataframe\n",
    "- **by index**: sort based on the index in the series OR based on the axis in a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pd.Series(np.arange(1, 14, 2), index=['c', 'a', 'b', 'k', 'i', 'm', 'e'])\n",
    "\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort by index in descending order\n",
    "\n",
    "s = s.sort_index(ascending=False)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort by values in ascending\n",
    "\n",
    "s.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.arange(1, 41, 3).reshape(7, 2), index=[3, 1, 7, 11, 32, 9, 0])\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort by index along axis = 1\n",
    "\n",
    "df.sort_index(axis=1, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort by index along axis=0\n",
    "\n",
    "df.sort_index(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort by values along axis = 1, but a key must be specified\n",
    "\n",
    "df.sort_values(axis=1, by=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---|||\n",
    "# Chapter 6: Data Loading, Storage, and File Formats\n",
    "\n",
    "---|||\n",
    "### Loading/Reading of Data into a Dataframe Object\n",
    "\n",
    "The read_[type] is meant to convert data stored on disk to a Dataframe object.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading a csv formatted data\n",
    "\n",
    "file_path = \"../datasets/Employees.csv\"\n",
    "\n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the index column to Unnamed\n",
    "df = df.set_index('Unnamed: 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove index name\n",
    "df.index.name = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this multiple operations could be done directly when loading the file\n",
    "\n",
    "df = pd.read_csv(file_path, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---|||\n",
    "# Using **read_table** as a general method for loading text data\n",
    "\n",
    "While *read_csv* is specific to CSV formatted files, one can use *read_table*, and indicate how the data is formatted in an optional seperator key (sep).\n",
    "\n",
    "The seperator between data points for CSV-formatted file is a comma (,)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use read_tables to open the Employee CSV data\n",
    "\n",
    "df = pd.read_table(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the format method in the seperator\n",
    "# remember the index column\n",
    "\n",
    "df = pd.read_table(file_path, sep=',', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"../datasets/pydata_datasets/babynames/yob1881.txt\"\n",
    "\n",
    "# suppose we don't know how the data is formatted, we use the read_table to peek\n",
    "\n",
    "df = pd.read_table(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since it is comma seperated, we use the read_csv file\n",
    "\n",
    "df = pd.read_csv(file_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---|||\n",
    "### Using the header key\n",
    "\n",
    "\n",
    "It is used to specify whether the data has column names(header) associated with it. Whenever the header argument is still specified, then;\n",
    "\n",
    "- it value is either 0 or NONE\n",
    "- the default column names (0, 1, 2, 3, ...) based on the inferenced number of columns \n",
    "\n",
    "When the **names** key is specified, header must be specified with...\n",
    "\n",
    "- None: if column names are not present\n",
    "- 0: if they are present, but will be renamed (using names=[val1, val2, ...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# something is amiss, the column names are part of the data\n",
    "# so we specify that the header (=> column names) is/are absent\n",
    "\n",
    "df = pd.read_csv(file_path, header=None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---|||\n",
    "### Setting the column names\n",
    "\n",
    "This can be achieved by passing a values to the **names** optional parameter in the read_csv method.\n",
    "\n",
    "There are implications if the length of the passed values exceed those in the data file. In this case, the name is added and populated with NAN values\n",
    "\n",
    "If the length of the passed values were less than those in the data file, then the the columns not accounted for are used to index the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to be even more pragmatic, by specify the names we want\n",
    "# instead of using the default integer column indexing\n",
    "\n",
    "df = pd.read_csv(file_path, header=None, names=['Name', 'Sex', 'Count'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"../datasets/pydata_datasets/haiti/Haiti.csv\"\n",
    "\n",
    "# peek into the data\n",
    "\n",
    "df = pd.read_csv(file_path)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the count of each columns of the dataframe\n",
    "\n",
    "df.count() \n",
    "\n",
    "# observe that the total length is 3593"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm the observation using the shape\n",
    "\n",
    "df.shape\n",
    "\n",
    "# its true; there are 3593 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# is the serial number a numbering format or special?\n",
    "\n",
    "# sort by index\n",
    "df.sort_values(by='Serial').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read*?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the values of serial to start from 1, by subtracting 3\n",
    "\n",
    "# since we have a dataframe, and we want to apply and operation on...\n",
    "# each values in the Serial column, we use the map method\n",
    "\n",
    "f = lambda x: x - 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Serial = df.loc[:, 'Serial'] - 3  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reindex by Serial\n",
    "\n",
    "df.set_index('Serial', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# an interesting data set\n",
    "file_path = \"../datasets/pydata_datasets/movielens/movies.dat\"\n",
    "\n",
    "# peek\n",
    "pd.read_table(file_path).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "\n",
    "- the file has no header: set header to [name, year, genre]\n",
    "- we have three seperator :: to get the title and genre, () to get the year\n",
    "- we should replace the |, with something better , a space\n",
    "\n",
    "- the index of the data should be the first column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_table(\n",
    "    file_path,\n",
    "    header=None,\n",
    "    names=['num', \"title\", \"year\", \"genre\", ''],\n",
    "    sep='(\\d+)::([^:]+)\\s\\((\\d+)\\)::(.*)',\n",
    "    engine='python',\n",
    "    keep_default_na=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---|||\n",
    "## Handling Missing Values \n",
    "\n",
    "It is possible to pass a string or sequence of strings, as values to the **na_values** key, which would marks any occurence of the string(s) as missing values (NAN, NA...) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets set any Abel and Echo as sentinel value\n",
    "\n",
    "df = pd.read_csv(\"../datasets/Employees.csv\", na_values=['Able', 'Echo'])\n",
    "\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-index\n",
    "\n",
    "df.set_index('Unnamed: 0', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename index to empty \n",
    "df.rename_axis(index=\"\", inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---|||\n",
    "### **na_values** for all or specific column(s)\n",
    "\n",
    "It is possible to specify that when a given data, as specified in the **value(s)** passed to the **na_values** key, then the data would be marked as sentinel i.e. the data would be marked as missing if any is found in the dataframe.\n",
    "\n",
    "It is also possible to specify that we want the matches in specific columns by passing a dictionary, containing a **key-value** pair to **na_values**, such that;\n",
    "\n",
    "- the key is the specific column name where we want to mark a certain data as sentinel\n",
    "\n",
    "- the value(s) is/are the data to mark as sentinel in the specified column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we could have set the sentinel by selecting the specific column\n",
    "\n",
    "# lets set any Abel and Echo as sentinel value in the Name column\n",
    "#and all the values less than four (4) in the YearOfService column\n",
    "\n",
    "df = pd.read_csv(\n",
    "    \"../datasets/Employees.csv\", \n",
    "    na_values={\n",
    "        'Name': ['Able', 'Echo'],\n",
    "        'YearOfService': [0, 1, 2, 3]       # values less than 4\n",
    "        },\n",
    "\n",
    "    # when names is specified, header must be specified with...\n",
    "    #   None: if column names are not present\n",
    "    #   0: if they are present, but will be renamed (using names=[])\n",
    "    header=0, # this allows us to specify no column\n",
    "    names= [\"\", \"Department\", \"Name\", \"YearOfService\"], # rename column\n",
    "    index_col=0\n",
    ")\n",
    "\n",
    "df.sort_values(by=['Name'], ascending=True).tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---|||\n",
    "### Converter Optional Parameter\n",
    "\n",
    "This is a **read_csv, and read_table** optional parameter, that take a dictionary as value.\n",
    "\n",
    "Its purpose is to apply a **function/mapping f** specified as value to a **key representing the column name, that the function should apply the mapping** to every values in the specified column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_table(\n",
    "    \"../datasets/a.txt\",\n",
    "    header=0,\n",
    "    names=[\"Deparment\", \"Name\", \"YearOfService\"],\n",
    "    sep=','\n",
    "    # index_col=0\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on the observation above, lets change the NAN values in Name to 'MO'abs\n",
    "\n",
    "f = lambda x: ('Mo' if not x else x )\n",
    "# def f(x):\n",
    "    # if x\n",
    "df = pd.read_table(\n",
    "    \"../datasets/a.txt\",\n",
    "    header=0,\n",
    "    names=[\"Deparment\", \"Name\", \"YearOfService\"],\n",
    "    sep=',',\n",
    "    converters={'Name': f}\n",
    "    # index_col=0\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---|||\n",
    "### Reading Text Files in Pieces\n",
    "\n",
    "It is optimal, when reading data in large files, to read the data from the file in small pieces OR iterate through smaller chunks of the file\n",
    "\n",
    "One way this can be achieved is to specify the number of rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the display to more compact format\n",
    "\n",
    "pd.options.display.max_rows = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"../datasets/pydata_datasets/movielens/movies.dat\"\n",
    "\n",
    "# take a look at the resulting dataframe for example\n",
    "df = pd.read_table(\n",
    "    file_path,\n",
    "    header=None,\n",
    "    names=['num', \"title\", \"year\", \"genre\", ''],\n",
    "    sep='(\\d+)::([^:]+)\\s\\((\\d+)\\)::(.*)',\n",
    "    engine='python',\n",
    "    keep_default_na=False,\n",
    ")\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 3883 rows.\n",
    "\n",
    "Suppose it was very large, then it would take a lot longer to parse the entire data into the dataframe. One way to avoid this is to specify the **number of rows** we want from the file.\n",
    "\n",
    "By passing the **nrows** keys, the read_[format] will stop when it reaches the number of rows value specified. This is especially useful when one wants to quickly examine data in a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instead of reading the entire file, we could specify the number of rows\n",
    "\n",
    "file_path = \"../datasets/pydata_datasets/movielens/movies.dat\"\n",
    "\n",
    "df = pd.read_table(\n",
    "    file_path,\n",
    "    header=None,\n",
    "    names=['num', \"title\", \"year\", \"genre\", ''],\n",
    "    sep='(\\d+)::([^:]+)\\s\\((\\d+)\\)::(.*)',\n",
    "    engine='python',\n",
    "    keep_default_na=False,\n",
    "    nrows=3\n",
    ")\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---|||\n",
    "### Reading in chunks, by specifying the chunksize\n",
    "\n",
    "When the chunksize value is specified, an iterator over the chunks of data in the file is returned.\n",
    "\n",
    "Here, if the **chunksize == p**, then p amount of rows will be returned everytime we call **next** on the iterator (or we iterate using for loop), until, there are no more data to iterate over\n",
    "\n",
    "More interestingly, we can call the **get_chunks(size=val)** method, and specify a certain **val**, over the iterator returned, to\n",
    "\n",
    "- read less than the value of rows that would have been returned in a given iteration,  **if val < p**\n",
    "\n",
    "- read more than the value of rows that would have been returned in a given iteration, **if val > p**\n",
    "\n",
    "- read a given number of rows from a given iteration, based on the value of the specified size in the **get_chunks** method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"../datasets/pydata_datasets/movielens/movies.dat\"\n",
    "\n",
    "# specify the chunksize to get an iterator\n",
    "\n",
    "chunk = pd.read_table(\n",
    "    file_path,\n",
    "    header=None,\n",
    "    names=['num', \"title\", \"year\", \"genre\", ''],\n",
    "    sep='(\\d+)::([^:]+)\\s\\((\\d+)\\)::(.*)',\n",
    "    engine='python',\n",
    "    keep_default_na=False,\n",
    "    chunksize=20\n",
    ")\n",
    "\n",
    "# in this iteration get the firs 20 rows\n",
    "next(chunk)\n",
    "\n",
    "# in this iteration, we get 22 rows; more than the chunksize specified\n",
    "print(chunk.get_chunk(size=22).shape)\n",
    "\n",
    "# in this iteration, we get the default chunksize specfied; 20\n",
    "print((next(chunk).shape))\n",
    "\n",
    "# in this iteration, we get 10 rows; less than the chunksize specified\n",
    "print(chunk.get_chunk(size=10).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"../datasets/pydata_datasets/movielens/movies.dat\"\n",
    "\n",
    "# specify the chunksize to get an iterator\n",
    "\n",
    "chunk = pd.read_table(\n",
    "    file_path,\n",
    "    header=None,\n",
    "    names=['num', \"title\", \"year\", \"genre\", ''],\n",
    "    sep='(\\d+)::([^:]+)\\s\\((\\d+)\\)::(.*)',\n",
    "    engine='python',\n",
    "    keep_default_na=False,\n",
    "    chunksize=10\n",
    ")\n",
    "\n",
    "# print the number of iterations it takes to read the entire file\n",
    "count = 0\n",
    "tot_rows = 0\n",
    "for piece  in chunk:\n",
    "    count += 1\n",
    "    tot_rows += piece.shape[0]\n",
    "    # print((piece.shape))\n",
    "\n",
    "'It took {0} iterations to read {1} number of rows'.format(count,  tot_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---|||\n",
    "### Writing Data To Text Format\n",
    "\n",
    "Using the **to_[format]**, we can save a dataframe data to disk based on the format specified\n",
    "\n",
    "\n",
    "dataframe.to_[format]([name.[format])\n",
    "\n",
    "\n",
    "- dataframe.to_excel([name.xlsl]): to excel format\n",
    "- dataframe.to_csv([name.csv]): to csv formatted\n",
    "- dataframe.to_json([name.json]): to json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"../datasets/Employees.csv\"\n",
    "\n",
    "df = pd.read_csv(\"../datasets/Employees.csv\", index_col=0)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# to json; the record format omit the index \n",
    "\n",
    "df.to_json(\"employee.json\", \"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to csv fomat, \n",
    "# but instead of the delimeter being comma, we use |\n",
    "\n",
    "df.to_csv(\"employee.csv\", sep=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# observe the following data\n",
    "file_path = \"a.txt\"\n",
    "\n",
    "# since it is a .txt file, we use the read_table format\n",
    "# actually, we could have used a read_csv, since we know it is comma seperated\n",
    "\n",
    "df = pd.read_table(file_path, sep=',', index_col=0)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "\n",
    "There are missing values in the Name column, to replace this, missing values with another value, we could have used the **converter** method while reading the file.\n",
    "\n",
    "However, we want to save this data to disk, and replace any missing values with **another name**.\n",
    "\n",
    "To achieve the above, we pass the **value** to **na_rep** key. This value will  stand in for the missing values in the dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"employee_NULL_for_NA.csv\", sep='|', na_rep=\"NULL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we could also remove the column labels by passing a header as false\n",
    "df.to_csv(\"employee_NULL_for_NA_w-o_header.csv\", sep='|', na_rep=\"NULL\", header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we could also remove the index by passing an index key as false\n",
    "df.to_csv(\"employee_NULL_for_NA_w-o_header_w-o_index.csv\", sep='|', na_rep=\"NULL\", header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instead of writing the output to a file, by passing the file name,\n",
    "# we could output the result into standard output, which will just print the result\n",
    "\n",
    "# get the stdout\n",
    "from sys import stdout\n",
    "\n",
    "df.to_csv(stdout, sep='|', na_rep=\"NULL\", header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can also indicate which columns we are interested in\n",
    "\n",
    "# we could also remove the index by passing an index key as false\n",
    "df.to_csv(stdout, sep='|', na_rep=\"NULL\", index=False, columns=[\"Department\", \"YearOfService\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---|||\n",
    "### Binary Data Formats\n",
    "\n",
    "One can take data in a different format and store it in binary format, in a process known as serialization\n",
    "\n",
    "The reverse is known as deserialization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read a csv file and save it in binary format\n",
    "\n",
    "f = lambda x: \"NULL\" if not x else x\n",
    "\n",
    "df = pd.read_table(\"a.txt\", index_col=['Unnamed: 0'], sep=',', converters={'Name': f})\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store it to binary format\n",
    "\n",
    "df_bin = df.to_pickle(\"employee_NULL_w-o_header_to_binary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the binary file\n",
    "\n",
    "df = pd.read_pickle(\"employee_NULL_w-o_header_to_binary\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---|||\n",
    "### Storage Formats\n",
    "\n",
    "HDF5: Hierachical Data Format\n",
    "\n",
    "This is a file format used for **storing large quantities** of scientific array data. \n",
    "\n",
    "An HDF5 file can store multiple datasets, and metadata as a key-value pair. Interestingly, it supports the compression of those file, using **a variety of compression modes**, so that data with repeated pattern are stored efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store a dataframe in HDF5 format\n",
    "\n",
    "# create the dataframe\n",
    "df = pd.DataFrame({'a': np.random.randn(100)})\n",
    "\n",
    "# store as hdf\n",
    "store = pd.HDFStore('data.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the dataframe\n",
    "\n",
    "store['obj1'] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store['obj1_col'] = df['a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(store.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve the df\n",
    "\n",
    "store['obj1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retreive the data in col a\n",
    "\n",
    "store['obj1_col']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---|||\n",
    "### Storage Schema\n",
    "\n",
    "Two schema (i.e. mode of storing data) are supported by HDF5Store;\n",
    "\n",
    "- fixed: fast, but doen't support query operations\n",
    "\n",
    "- table: slow, but supports query operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame({'b': np.random.randn(40), 'e': np.random.randn(40)})\n",
    "\n",
    "store.put('obj2', value=df2, format='table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note, if storage schema is \"fixed, then this query operation will fail\n",
    "store.select('obj2', where=['index >= 10 and index <= 15'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# close the storage\n",
    "\n",
    "store.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm close\n",
    "\n",
    "store['obj1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# performing the same operation using read_hdf5, and to_hddf. \n",
    "a = np.arange(20).reshape((10, 2))\n",
    "\n",
    "df  = pd.DataFrame(a, columns=['a', 'b'])\n",
    "\n",
    "df.to_hdf('rdata.h5', 'obj', format='table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_hdf('rdata.h5', 'obj', where= ['index >=3', 'columns = a' ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---|||\n",
    "# Chapter 7: Data Cleaning and Preparation\n",
    "\n",
    "> Note: cleaning and preparing data should be done within the context of the analysis of the data, how the data is collected\n",
    "\n",
    "**Steps Involved in Cleaning and Preparing Data for Analysis**\n",
    "\n",
    "- **Filtering**:\n",
    "\n",
    "    - duplicates\n",
    "    - missing values\n",
    "    - badly represented string values\n",
    "    - detecting outliers\n",
    "\n",
    "- **Transformation**:\n",
    "    \n",
    "    - mapping data to new form\n",
    "    - replacing data \n",
    "    - transforming badly represented string values\n",
    "    - renaming indexes (rows and column index)\n",
    "\n",
    "Cleaning involves removing data-points that will have adverse effect on the kind of analysis we want to perform on the data. This involve;\n",
    "\n",
    "- missing data\n",
    "- duplicates fata\n",
    "- badly processed data (e.g. strings that contains special characters)\n",
    "\n",
    "Preparation on the other hands comes after cleaning. It involves processing the cleaned data to a form that is suitable for the analysis to be be performed. This includes;\n",
    "\n",
    "- joining\n",
    "- grouping\n",
    "\n",
    "\n",
    "---|||\n",
    "### Querying A Series or Dataframe for Missing Data\n",
    "\n",
    "- isnull: returns an array of boolean with True specified for sentinel value\n",
    "- notnull: returns an array of boolean with False specified for sentinel value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import nan as NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'a': [1, NA, 3, 6, 9, NA],\n",
    "    'b': [2, 4, NA, NA, 8, 10],\n",
    "    'c': [NA, NA, 10, NA, 20, NA ]\n",
    "}\n",
    "\n",
    "# create a dataframe\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the isnull to check for all the sentinel values in column a\n",
    "\n",
    "df['a'].isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check all the non-null values in column b\n",
    "\n",
    "df['b'].notnull()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---|||\n",
    "### Filtering Missing Data\n",
    "\n",
    "**dropna**\n",
    "\n",
    "This by default drop any rows containing a sentinel value.\n",
    "\n",
    "\n",
    "Customization:\n",
    "\n",
    "- axis: 0 (rows), 1(columns)\n",
    "- how: \"all\" or \"any\"\n",
    "- thresh: indicates the number of non-sentinel values required to NOT drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all the rows containing missing data\n",
    "\n",
    "df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a new column containing non-sentinel values\n",
    "\n",
    "df['d'] = 33\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instead of remove rows by default, remove columns containing sentinels\n",
    "\n",
    "df.dropna(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the number of non-sentinel values as condition for dropping a row\n",
    "\n",
    "# only remove rows that contains less than 2 non-sentinel values\n",
    "\n",
    "df.dropna(thresh=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use axis 1\n",
    "\n",
    "df.dropna(axis=1, thresh=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a new index to the dataframe\n",
    "\n",
    "df = pd.DataFrame(df, index=[0, 1, 2, 3, 4, 5, 6])\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify that only rows where all values are sentinel be dropped\n",
    "\n",
    "df.dropna(how=\"all\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add an e column containing missing values\n",
    "\n",
    "df = pd.DataFrame(df, columns=['a', 'b', 'c', 'd', 'e'])\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify that only columns where all values are sentinel be dropped \n",
    "\n",
    "df.dropna(axis=1, how=\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---|||\n",
    "### Filling Missing Data\n",
    "\n",
    "The consideration here is to replace the missing data with another value, so as to avoid discarding non-sentinel data from the series or dataframe\n",
    "\n",
    "**fillna**: used to fill sentinel values in a dataframe\n",
    "\n",
    "Customization:\n",
    "\n",
    "method: \"ffill\", \"bfill\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill all sentinel values with a 0\n",
    "\n",
    "df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can chose how to fill sentinel values by columns\n",
    "\n",
    "df.fillna({'a': 22, 'b': 111, 'c': 999, 'd': 1000, 'e': 333})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can chose to fill sentinel values in each columns by the last valid value before the sentinel in that column\n",
    "\n",
    "df.fillna(method=\"ffill\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can also specify the number of values we want to forward fill (ffill)\n",
    "\n",
    "# we only fill one sentinel value that follow each other consecutively using the last valid value\n",
    "\n",
    "df.fillna(method=\"ffill\", limit=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used bfill to fill the sentinel values\n",
    "# using the first valid value after the sentinel value\n",
    "\n",
    "df.fillna(method=\"bfill\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---|||\n",
    "### Data Transformation\n",
    "\n",
    "This involves transforming the data to produce a standardized version of the data that can be used in analysis. Transforming the data includes;\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\"a1\": [3, 4, 6, 3, 4, 6, 3], \"a2\": [3, 4, 6 ,3, 4, 6, 3], \"a3\": [1, 4, 6, 1 ,4, 6, 5]}\n",
    "\n",
    "# create the dataframe with only four unique index \n",
    "\n",
    "df = pd.DataFrame(data, index=[\"one-1\", \"two-1\", \"three-1\", \"one-2\", \"two-2\", \"three-2\", \"four-1\"])\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for duplicates data in all the columns\n",
    "# note, the index is not considered in duplicated operation by default\n",
    "\n",
    "df.duplicated()\n",
    "\n",
    "# indicates that 5th and 6th rows are duplicated\n",
    "\n",
    "# ---\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select duplicates based on a subset of columns\n",
    "\n",
    "# check for duplicates in a1 and a2\n",
    "\n",
    "df.duplicated(subset=[\"a1\", \"a2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select duplicates based on a subset of columns\n",
    "\n",
    "# check for duplicates in a1 and a3\n",
    "\n",
    "df.duplicated(subset=[\"a1\", \"a3\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select duplicates based on a subset of columns\n",
    "\n",
    "# check for duplicates in a2 and a3\n",
    "\n",
    "df.duplicated(subset=[\"a2\", \"a3\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---|||\n",
    "### Filtering out Duplicated Data\n",
    "\n",
    "**drop_duplicates**\n",
    "\n",
    "By default, it drops the duplicated rows; here the elements in all the columns must match. This can be customized to choose a subset of the columns that will be considered before dropping the rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicated data, whose values match in all the columns\n",
    "# this method does not remove both the original and duplicates... \n",
    "# it only removes the duplicated data\n",
    "\n",
    "df.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can customize this to remove both the orginal and duplicates\n",
    "\n",
    "df.drop_duplicates(keep=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we could instead keep the last duplicates\n",
    "\n",
    "df.drop_duplicates(keep=\"last\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or we could keep the first which is the default\n",
    "\n",
    "df.drop_duplicates(keep=\"first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets use the columns subset to drop duplicates\n",
    "\n",
    "# any duplicate in a1, and a2, should be used to drop the duplicates\n",
    "\n",
    "df.drop_duplicates(subset=[\"a1\", \"a2\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# any duplicate in a2, and a3, should be used to drop the duplicates\n",
    "\n",
    "df.drop_duplicates(subset=[\"a2\", \"a3\"]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---|||\n",
    "### Transformation Using A Function\n",
    "\n",
    "Instead of dropping missing data, and losing other valid datapoints with it, we may wish to transorm the sentinel values to another value that won't have adverse effect on the analysis. \n",
    "\n",
    "A function or mapping is specified to transform data in a series or columns in a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "\n",
    "data = {\n",
    "    \"country\": ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h'],\n",
    "    \"population\": [20, 10 ,1, 3, 1.5, 4, 2, 9]\n",
    "}\n",
    "\n",
    "file_path = \"a.txt\"\n",
    "\n",
    "df = pd.read_table(file_path, sep=',', index_col=0)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill all the na values with \"null\"\n",
    "\n",
    "df.fillna({\"Name\": \"null\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let standardize the data so that all the data in \n",
    "# department and name are in uppercase\n",
    "\n",
    "df[\"Department\"] = df.loc[:, [\"Department\"]].applymap(lambda x: x.upper())\n",
    "df[\"Name\"] = df.loc[:, [\"Name\"]].applymap(lambda x: x.upper())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---|||\n",
    "### Stringifying a Series\n",
    "\n",
    "This will convert all the data in the series to  their string representation. When this succeed, string methods can be applied on each data in the series.\n",
    "\n",
    "> Note: this applies only to a series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we could have achieved the same thing as above by\n",
    "# first converting it to a string format, and then use strings method\n",
    "\n",
    "df.loc[:, \"Department\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---|||\n",
    "### Series Map method\n",
    "\n",
    "This accepts a dictionary or function that takes a datapoint and map it to another value; IF the argument is a:\n",
    "\n",
    "**Dictionary**\n",
    "\n",
    "==series.map([dictionary={key:value}])==\n",
    "\n",
    "Then it would map each **key** that is contained in the **series instance** to a new **value**. IF the key is however not present in the series, it would be mapped to nan.\n",
    "\n",
    "**Function**\n",
    "\n",
    "==series.map(function)==\n",
    "\n",
    "Then the function would be apply to each datapoint to produce the corresponding output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Department\"] = df[\"Department\"].map(lambda datapoint: datapoint.lower())\n",
    "df[\"Name\"] = df[\"Name\"].map(lambda datapoint: datapoint.lower())\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets map marketing - sales, engineering - tech and accounting - finance\n",
    "\n",
    "df[\"Department\"].map({\"marketing\": \"sales\", \"engineering\": \"tech\", \"accounting\": \"finance\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we could have done the above using a custom function\n",
    "\n",
    "def department_mapping(datapoint: str):\n",
    "    # accounting\n",
    "    if datapoint.startswith('a'):\n",
    "        return \"finance\"\n",
    "\n",
    "    # engineering\n",
    "    if datapoint.startswith(\"e\"):\n",
    "        return \"tech\"\n",
    "\n",
    "    # marketing\n",
    "    # since we know our datapoint falls into these three categories,\n",
    "    # ... and that there are no sentinels in this column, ... \n",
    "    # we know that marketing is the only category that hasn't been checked. \n",
    "    return \"sales\"\n",
    "    # if datapoint.startswith(\"m\"):\n",
    "        # return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the funtion\n",
    "df[\"Department\"].map(department_mapping).str.capitalize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---|||\n",
    "### Data Replacement\n",
    "\n",
    "This generally involves substituting another value for sentinels and non-sentinels datapoints.\n",
    "\n",
    "This is more flexible than using the **fillna** method\n",
    "\n",
    "**replace(to_replace, value(opt))**\n",
    "\n",
    "**Single Replacement**\n",
    "\n",
    "- **datapoint** to a **value**\n",
    "\n",
    "**Multiple Replacement**\n",
    "\n",
    "- a **list of datapoints** to a **list of values**: both lists length must match\n",
    "\n",
    "- we can use a dictionary such that:\n",
    "    - **keys** specifies the datapoints to replace, and\n",
    "    - **values** specfies what to replace the datapoints with\n",
    "\n",
    "    > if this is intended, then the **value optional parameter** would not be passed\n",
    "\n",
    "    IF the value parameter is however specified, this imply that, we want to search a dataframe having its column as the **keys** specified, and to search for any datapoints equal to the **value** specified, and replace those datapoints to the one given in the **list of values specified in the optional value parameter.\n",
    "\n",
    "- we can also use a dictionary that contains another dictionary such that;\n",
    "\n",
    "    - the outer dictionary is such that its;\n",
    "        - **keys**: represent the columns to perform the replacement operations\n",
    "        \n",
    "        - **values**: is the inner dictionary containing;\n",
    "            - **keys** representing the datapoints to replace\n",
    "            - **values** representing what to replace the datapoints with\n",
    "\n",
    "    > Here also, the **value optional parameter** will not be specified\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use replace to perform the same mapping above\n",
    "\n",
    "df.replace({\n",
    "    \"Department\": \n",
    "        {\n",
    "            \"accounting\": \"finance\",\n",
    "            \"marketing\": \"sales\",\n",
    "            \"engineering\": \"tech\"\n",
    "        },\n",
    "    \"Name\": {\n",
    "            \"null\": \"Mo\"\n",
    "        }\n",
    "        \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using another dictionary format\n",
    "\n",
    "df.replace({\"Name\": \"null\" }, value= \"Xango\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using list\n",
    "\n",
    "df.replace(\n",
    "    [\"null\", 'engineering', 'accounting', 'marketing'],\n",
    "    [\"Xango\", \"Techers\", \"Salers\", \"Financers\"]\n",
    " )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---|||\n",
    "### Renaming Axis Indexes\n",
    "\n",
    "The axis indexes can be replaced by calling the map method an instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the index\n",
    "\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename the index to uppercase\n",
    "\n",
    "to_uppercase = lambda x: x.upper()  \n",
    "\n",
    "df.columns = df.columns.map(to_uppercase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the rename method instead creates a copy\n",
    "\n",
    "df.rename(columns=str.lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a dictionary instead\n",
    "# remember the columns are already in uppercase, hence making the keys in uppercase\n",
    "df.rename(columns={\n",
    "    \"department\".upper(): \"Department\",\n",
    "    \"name\".upper(): \"Name\",\n",
    "    \"yearofservice\".upper(): \"YearOfService\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---|||\n",
    "# Discretizing and Binning Data\n",
    "\n",
    "This allows separating data into a given discrete value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---|||\n",
    "### **cut** function\n",
    "\n",
    "Takes a one-dimensional array and a segment that determines how the data would be divided.\n",
    "\n",
    "It returns a series with its values as the segment where each datapoints falls in\n",
    "\n",
    "NOTE:\n",
    "\n",
    "- if an array is passed to cut for binning, a **Categorical** object is returned, and we can access the index of the categories (codes) and categories by using the attributes directly\n",
    "\n",
    "- if a series is passed, then a series is returned. To access the codes and and categories attribut, we must first convert the returned series to a categorical object using the **cat** attribute. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide the year of service into bins 1 to 2, 3 to 7, 8 to 12, and 13 to higher values\n",
    "\n",
    "bins = [0, 2, 7, 12, 20]\n",
    "\n",
    "category = pd.cut(df[\"YEAROFSERVICE\"], bins)\n",
    "category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the categories each datapoints falls into\n",
    "\n",
    "category.cat.categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that the categories are close to the right and open to the left; i.e. (., .]\n",
    "\n",
    "This can be changed by specfiying the **right** parameter in the cut instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the index of each category the data points falls into\n",
    "\n",
    "# 0 -> (0, 2], \n",
    "# 1 -> (2, 7]\n",
    "# 2 -> (7, 12]\n",
    "# 3 -> (12, 20]\n",
    "category.cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide the year of service into bins 1 to 2, 3 to 7, 8 to 12, and 13 to higher values\n",
    "\n",
    "# have a closed interval at the leftm and open to the right\n",
    "bins = [0, 2, 7, 12, 20]\n",
    "\n",
    "category = pd.cut(df[\"YEAROFSERVICE\"], bins, right=False)\n",
    "category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get how many values falls in each segments using value_counts\n",
    "\n",
    "pd.value_counts(category)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to pass an identifier to identify each segments\n",
    "\n",
    "This is achieved using the **label optional parameter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide the year of service into bins 1 to 2, 3 to 7, 8 to 12, and 13 to higher values\n",
    "\n",
    "bins = [0, 2, 7, 12, 20]\n",
    "\n",
    "# since we have four segment, we must pass a list having four label\n",
    "\n",
    "category = pd.cut(\n",
    "    df[\"YEAROFSERVICE\"],\n",
    "  bins, \n",
    "    labels=[\n",
    "        \"> 0 and <= 2\",\n",
    "        \"> 2 and <= 7\",\n",
    "        \"> 7 and <= 12\",\n",
    "        \">12 and <=20\"\n",
    "    ])\n",
    "\n",
    "category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view the category\n",
    "\n",
    "category.cat.categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can pass an integer for bin, and it will  compute equal length bins\n",
    "\n",
    "# divide the year of service into bins 4 to higher values\n",
    "\n",
    "category = pd.cut(df[\"YEAROFSERVICE\"], bins=4)\n",
    "category.cat.categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constrain the floating point values to a given precision\n",
    "# by passing a precision parameter\n",
    "\n",
    "data = np.random.random(20)\n",
    "\n",
    "category = pd.cut(data, bins=4, precision=3)\n",
    "category.categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---|||\n",
    "### Binning Using Quantiles using **qcut**\n",
    "\n",
    "\n",
    "In this case, it will bin the datapoints into the specified integers, where:\n",
    "\n",
    "- 2 ==> into two\n",
    "- 4 ==> into quartiles\n",
    "- 100 ==> percentiles\n",
    "\n",
    "e.t.c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide the YearOf Service into percentiles\n",
    "\n",
    "category = pd.qcut(df[\"YEAROFSERVICE\"], 100, duplicates=\"drop\")\n",
    "\n",
    "category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check how many categories\n",
    "\n",
    "category.cat.categories.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---|||\n",
    "### Detecting and Filtering Outliers\n",
    "\n",
    "**sign method(it is a numpy method)** : takes a dataframe or series, and produce\n",
    "-  (+1) if the element is postive\n",
    "\n",
    "-  (-1) if the element is postive\n",
    "\n",
    "    and return the corresponding series or dataframe\n",
    "\n",
    "**sample method**: takes a series or dataframe, and generate a random sample based on the number passed as argument. Additionally, the replace boolean keyword can modified such that IF set to;\n",
    "\n",
    "- True: the sample will be generated **with replacement**: the sample size can be greater than the population size\n",
    "\n",
    "- False: the sample will be generated **without replacement**: the sample size must be less than the population size\n",
    "\n",
    "**take method**: takes an array of row index, and produce the series or data frame with the passed row indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sign(pd.Series([-3, 4, -7, 8]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.random.randn(20).reshape((5, 4))\n",
    "\n",
    "df = pd.DataFrame(data, columns=['a', 'b', 'c', 'd'])\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sign\n",
    "\n",
    "np.sign(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take method\n",
    "\n",
    "df.take([0, 3, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take 3 samples without replacement: \n",
    "# in this case the sample(n=3) cannot be greater than the population (total n = 5)\n",
    "\n",
    "df.sample(n=3, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take 10 samples with replacement\n",
    "# in this case the sample(n=10) can be greater than the population (total n = 5)\n",
    "\n",
    "df.sample(n=10, replace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---|||\n",
    "### Computing Indicator/Dummy Variables\n",
    "\n",
    "Converting a **categorical variable** into a **dummy or indicator matrix** using the categorial variable: with 0 indicating absent, and 1 indicating present\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    {'key': ['i', 'j', 'k', 'j', 'k', 'i', 'p', 'j', 'p'],\n",
    "     'data': range(9)\n",
    "})\n",
    "\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the indicator matrix\n",
    "\n",
    "# this creates a matrix, with 0 indicating absent, and 1 indicating present\n",
    "\n",
    "pd.get_dummies(df[\"key\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a prefix to the categorical variable\n",
    "\n",
    "dummy_matrix = pd.get_dummies(df['key'], prefix='category')\n",
    "\n",
    "dummy_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_dummy = df[['data']].join(dummy_matrix)\n",
    "df_with_dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indicator matrix using the movielens dataset\n",
    "\n",
    "file_path = \"../datasets/pydata_datasets/movielens/movies.dat\"\n",
    "\n",
    "# movies = pd.read_table(file_path, header=None, names=[\"movie_id\", \"title\", \"genre\"], sep=\"::\")\n",
    "movies = pd.read_table(file_path, header=None, names=[\"movie_id\", \"title\", \"genre\"])\n",
    "\n",
    "movies.iloc[37]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"../datasets/pydata_datasets/movielens/movies.dat\"\n",
    "f = lambda x: int(x) if not np.nan else x\n",
    "mov = pd.read_table(\n",
    "    filepath_or_buffer=file_path,\n",
    "    sep=r\"(\\d*)::\\s?(.+)\\((\\d*)\\)?::(.*|\\s)$\",\n",
    "    engine=\"python\",\n",
    "    header=None,\n",
    "    names=[\"num\", \"movie_id\", \"title\", \"year\", \"genre_1\", \"\"],\n",
    "    converters={\"year\": f, \"genre_2\": g}\n",
    ")\n",
    "\n",
    "mov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mov_cleaned = mov.dropna(how=\"all\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mov_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = mov.genre_1.str.strip()\n",
    "gen = gen.str.split(\"|\")\n",
    "\n",
    "gen.str.join(\", \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the unique genres\n",
    "\n",
    "genre = df[\"genre\"].str.strip()\n",
    "genre = genre.str.split('|')\n",
    "\n",
    "genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add all the result into a single list\n",
    "all_genre = []\n",
    "\n",
    "for x in genre:\n",
    "    all_genre.extend(x)\n",
    "len(all_genre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the unique ones\n",
    "\n",
    "uniq_genre = pd.unique(all_genre)\n",
    "\n",
    "uniq_genre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the indicator matrixe\n",
    "\n",
    "# remember, the categorical variable will be the columns of the matrix, and\n",
    "# the rows of the matrix will correspond to that of the entire dataset\n",
    "\n",
    "# create a zero_matrix, that will have the shape of the resulting indicator matrix\n",
    "zero_matrix = np.zeros((len(movies), len(uniq_genre)))\n",
    "\n",
    "# create the dataframe, with 0's, but the columns are the categorical variable (uniq_genre)\n",
    "dummies = pd.DataFrame(zero_matrix, columns=uniq_genre)\n",
    "dumies_copy = dummies.copy()\n",
    "dummies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, gen in enumerate(movies.genre):\n",
    "    indices = dummies.columns.get_indexer(gen.split('|'))\n",
    "    dummies.iloc[i, indices] = 1\n",
    "\n",
    "# this is the same as above\n",
    "# for i, genre in enumerate(movies.genre):\n",
    "#     genre = genre.split('|')\n",
    "\n",
    "#     dumies_copy.loc[i, genre] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_with_indicator = movies.join(dummies.add_prefix(\"Genre_\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_with_indicator.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have to iterate through the movies, genre, \n",
    "# and if a row has a given genre in its values, \n",
    "# we should add a 1 to the the column containing the genre\n",
    "\n",
    "movies.head()\n",
    "\n",
    "genre = movies.genre\n",
    "genre.str.contains('Animation')\n",
    "\n",
    "np.where(cond, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dumies_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---|||\n",
    "### String Manipulation & Regular Expression\n",
    "\n",
    "**Important String Methods**\n",
    "\n",
    "- *startswith and endswith*\n",
    "- *join*\n",
    "- *find*: returns -1 if it fails to find the requested data\n",
    "- *strip, rstrip and lstrip*: trim whitespaces, including newlines(\\n)\n",
    "- *replace*\n",
    "- *split*\n",
    "\n",
    "---|||\n",
    "#### Regular Expression\n",
    "\n",
    "- **Compiling a regular exepression**:\n",
    "\n",
    "Suppose we want to apply a regular expression method on different strings for the purpose of processing the strings, then it is better, for efficiency, to compile the regular expression, before calling the method on the strings. \n",
    "\n",
    "\n",
    "- **Regex Methods**\n",
    "\n",
    "0. **compile**: this allows us to compile the regex, and possible flags, so that we can utilize it with different methods (as given below) by saving us the compilation step. Consequently, it saves us of CPU cycles.\n",
    "\n",
    "1. **search**: returns an object that can be queried for the *starting* and *ending* position of the first match.\n",
    "\n",
    "2. **match**: it does the same thing as *search* above. But it starts matching at the beginning of the string, **unlike *search that scans through the string to find its first match**\n",
    "\n",
    "3. **findall**: this returns all the matches\n",
    "\n",
    "4. **split**: similar to Python native split method on strings, although this is now within the context of regex.\n",
    "\n",
    "5. **sub**: returns a new string with the matched occurences replaced with a new strings\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m \u001b[0mregex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendpos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m9223372036854775807\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m Matches zero or more characters at the beginning of the string.\n",
      "\u001b[0;31mType:\u001b[0m      builtin_method"
     ]
    }
   ],
   "source": [
    "regex.match?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the regular expression library\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"  foo      bar\\t  baz     \\tqux\"\n",
    "\n",
    "# compile the regular expression \"\\s+\", so we can use different methods on it\n",
    "regex = re.compile('\\s+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 2), match='  '>"
      ]
     },
     "execution_count": 505,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# return the position where the first match occurs\n",
    "\n",
    "regex.search(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', 'foo', 'bar', 'baz', 'qux']"
      ]
     },
     "execution_count": 506,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split on matches\n",
    "\n",
    "regex.split(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 2), match='  '>"
      ]
     },
     "execution_count": 507,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regex.match(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['  ', '      ', '\\t  ', '     \\t']"
      ]
     },
     "execution_count": 508,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find all the matches\n",
    "\n",
    "regex.findall(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'REPLACEMENT-TEXTfooREPLACEMENT-TEXTbarREPLACEMENT-TEXTbazREPLACEMENT-TEXTqux'"
      ]
     },
     "execution_count": 509,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regex.sub(\"REPLACEMENT-TEXT\", text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---|||\n",
    "### Vectorized String Functions In Pandas\n",
    "\n",
    "Pandas allows the use of native python string methods, and regex methods, through \"stringifying\" a column using the **str** attributes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'Dave': 'dave@google.com', 'Steve': 'steve@gmail.com','Rob': 'rob@gmail.com', 'Wes': np.nan}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.Series(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dave     dave@google.com\n",
       "Steve    steve@gmail.com\n",
       "Rob        rob@gmail.com\n",
       "Wes                  NaN\n",
       "dtype: object"
      ]
     },
     "execution_count": 516,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = '([A-Z0-9._%+-]+)@([A-Z0-9.-]+)\\\\.([A-Z]{2,4})'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dave     [(dave, google, com)]\n",
       "Steve    [(steve, gmail, com)]\n",
       "Rob        [(rob, gmail, com)]\n",
       "Wes                        NaN\n",
       "dtype: object"
      ]
     },
     "execution_count": 540,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.str.findall(pattern, flags=re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dave     True\n",
       "Steve    True\n",
       "Rob      True\n",
       "Wes       NaN\n",
       "dtype: object"
      ]
     },
     "execution_count": 543,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matches = data.str.match(pattern, flags=re.IGNORECASE)\n",
    "\n",
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dave       a\n",
       "Steve      t\n",
       "Rob        o\n",
       "Wes      NaN\n",
       "dtype: object"
      ]
     },
     "execution_count": 546,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.str.get(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
