{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---|||\n",
    "# Pandas  Introduction \n",
    "\n",
    "It is a Python library used for data manipulation, cleaning and processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The core data structure in pandas are Series (column like), and Dataframe(tabular like)\n",
    "---|||\n",
    "\n",
    "**Series**: one dimensional array-like object containing\n",
    "-  sequence of values, **P**\n",
    "-  an associated array of data labels, called its **index**\n",
    "\n",
    "> By default, the index ranges from 0 to the len(P) - 1 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notice the associated indices\n",
    "pd.Series([5, 10, 111, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a series with modified index, using its index parameter\n",
    "\n",
    "s = pd.Series([10, 20, 30, 40], index=['i', 'j', 'k', 'l'])\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain the index of a given series using the index attribute\n",
    "\n",
    "s.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---|||\n",
    "##### The index of a Series can be used to select data corresponding to the index\n",
    "\n",
    "They act as index to the data sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s['k'], s['i']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### NumPy-like operations can be used to manipulate a Series object\n",
    "\n",
    "The index remains unchanged, after the operation(s) is performed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s[s > 25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.exp(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query if the series contains a given index\n",
    "\n",
    "'j' in s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A good mental model is to think of a Series object as a dictionary of keys and values\n",
    "\n",
    "- the index are the keys\n",
    "- the data are the values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some states and their corresponding capitals in Nigeria\n",
    "sdata = {'lagos': 'ikeja', 'ogun': 'abeokuta', 'adamawa': 'lafia'}\n",
    "\n",
    "# create a series object from the data\n",
    "s = pd.Series(sdata)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the index\n",
    "\n",
    "s.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query if an index is contained in the series\n",
    "\n",
    "'lagos' in s, 'fct' in s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---|||\n",
    "\n",
    "#### Alter the indices of a Series in-place"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = ['l', 'o', 'a']\n",
    "\n",
    "# change the indices to index\n",
    "s.index = index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---|||\n",
    "#### Using the Dictionary to create the series will sort the data based on the keys\n",
    "\n",
    "This can be overriden by passing the same keys, and in whatever order to the index keyword\n",
    "\n",
    "> adding a key that doesn't belong in the dictionary will result in the key having a NAN value (a.k.a missing data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding more indices than data, results in NAN values\n",
    "\n",
    "# some states and their corresponding capitals in Nigeria\n",
    "sdata = {'lagos': 'ikeja', 'ogun': 'abeokuta', 'adamawa': 'lafia'}\n",
    "\n",
    "# using the keys to order the values\n",
    "s = pd.Series(sdata, index=['lagos', 'adamawa', 'ogun'])\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using a key that is not in the dictionary => 'fct\n",
    "# will add the key, but its value will benan\n",
    "\n",
    "\n",
    "s = pd.Series(sdata, index=['ogun', 'fct', 'adamawa', 'lagos'])\n",
    "\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---|||\n",
    "#### **isnull** and **notnull** method as a way of detecting missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find index with missing data in a series\n",
    "\n",
    "s.isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find index without missing data in the series\n",
    "\n",
    "s.notnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the query formats (functions and instance method) are equivalent\n",
    "\n",
    "\n",
    "(\n",
    "    pd.notnull(s) == s.notnull(),\n",
    "    \n",
    "    pd.isnull(s) == s.isnull()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---|||\n",
    "##### Arithmetic Operations on series with similar keys \n",
    "\n",
    "When performing arithmetic operations on different series with similar keys, the keys are used to align the data before the operation is performed element wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata = {'i': 11, 'j': 33, 'k': 23}\n",
    "bdata = {'a': 100, 'j': 23, 'b': 73, 'k': 1000} # keys j, k are in adata\n",
    "\n",
    "s1 = pd.Series(adata)\n",
    "s2 = pd.Series(bdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notice that not all the keys are the same\n",
    "\n",
    "# additional keys thar are not present in either index(es) ...\n",
    "# wi;; return nan values\n",
    "\n",
    "s1 + s2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|^|\n",
    "\n",
    "Notice that keys that doesn't match have NAN values returned\n",
    "\n",
    "---\n",
    "---|||\n",
    "#### Naming a Series \n",
    "\n",
    "It is possible to name a series object using the name attribute of kwarg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pd.Series(sdata, name='States and Capital in Nigeria')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the name using its index attributes\n",
    "\n",
    "s.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---|||\n",
    "# Dataframe\n",
    "\n",
    "The other core pandas object is the DataFrame representing a tabular-like data (like excel spreadsheet). It contains\n",
    "\n",
    "- ordered collections of columns; each column can contain different data type\n",
    "- a row and column index\n",
    " \n",
    "A good mental model is to think of Dataframe as a dictionary containing\n",
    "\n",
    "- **keys**: representing the columns, and its column index\n",
    "-\n",
    "- **value**:  Series object such that\n",
    "  -  the keys of the series represent the row index\n",
    "  - the values in the series represent the data keyed by its row and column index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a dataframe\n",
    "\n",
    "data = {'states': ['lagos', 'fct', 'ondo', 'oyo', 'plateau'], 'capital': ['ikeja', 'abuja', 'akure', 'ibadan', 'jos']}\n",
    "\n",
    "# the row index will automatically default to a number, \n",
    "# if we had used the dictionary as is.\n",
    "# but we pass one in the intialisation\n",
    "\n",
    "df = pd.DataFrame(data, index=['i', 'j', 'k', 'l', 'm'])\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe by passing list\n",
    "# and we can specify the column index...\n",
    "# in the order we want them to appear\n",
    "\n",
    "df = pd.DataFrame(\n",
    "  [('lagos', 'ikeja', 'SW'),\n",
    "   ('fct', 'abuja', 'NC'), \n",
    "   ('ondo', 'akure', 'SW'),\n",
    "   ('oyo', 'ibadan', 'SW'),\n",
    "   ('plateau', 'jos', 'SS')],\n",
    "  columns=['states', 'capitals', 'geographic_region'], index=['a', 'b', 'c', 'd', 'e']\n",
    ")\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we passed this directly, then the columns will be ... \n",
    "# ordered based on the keys - (capital, geographic_region, states)\n",
    "\n",
    "data = {\n",
    "    'states': ['lagos', 'fct', 'ondo', 'oyo', 'plateau'],\n",
    "    'capital': ['ikeja', 'abuja', 'akure', 'ibadan', 'jos'],\n",
    "    'geographic_region': ['SW', 'NC', 'SW', 'SW', 'SS']\n",
    "    }\n",
    "\n",
    "# notice the order of the columns indices (geographic.., states, capital)\n",
    "\n",
    "# also notice that the population index in columns ... \n",
    "# is not contained in the data, so its values will be nan\n",
    "\n",
    "df = pd.DataFrame(data, columns=['geographic_region', 'states', 'capital', 'population'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---|||\n",
    "#### **Head** or **Tail** Selecting the top or last few elements\n",
    "\n",
    "We can select the first few elements in the beginning or end of the dataframe using the **head** and **tail** method.\n",
    "\n",
    "By default they return the first or last five rows in the dataframe, however, we can pass-in a number to indicate the number of rows that should be returned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the first n values of a dataframe\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the column index using the column attributes\n",
    "\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---|||\n",
    "\n",
    "# Indexing a Dataframe\n",
    "\n",
    "Remember that the keys to  the dataframe are its columns index.\n",
    "\n",
    "When the dataframe object is indexed by the column name, a series containing the row index and its corresponding data is returned.\n",
    "\n",
    "Indexing can be carried out in two ways; \n",
    "- **dictionary indexing**: as key \n",
    "- **attribute indexing**: using the name of the column\n",
    "\n",
    "> the dictionary indexing format is more general, as it can also be used with column index with *space* in their name, which would have other_wise be invalid using attribute. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the data in the capital -> a series \n",
    "\n",
    "df['capital']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using attribute index\n",
    "\n",
    "df.geographic_region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the population column\n",
    "\n",
    "df.population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign values to the population column\n",
    "\n",
    "df.population = [20, 10, 3, 4, 1]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remember broadcasting?\n",
    "\n",
    "df.population = 1\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---|||\n",
    "\n",
    "#### Adding a new series to an existing dataframe object\n",
    "\n",
    "Add a new column then;  add a series to a new column in the dataframe\n",
    "\n",
    "Satisfy the following\n",
    "\n",
    "- The length of the series data must match the those in the dataframe\n",
    "- The index length of the series must match those in the dataframe, \n",
    "- the index names,that matches those in the dataframe will be aligned\n",
    "- the index name that doesn't match will be NAN\n",
    "\n",
    "\n",
    "> Note, if the index length, those of the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the row index names\n",
    "\n",
    "df.index = ['one', 'two', 'three', 'four', 'five']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a series\n",
    "\n",
    "\n",
    "# the max length of the series must match those of the dataframe\n",
    "\n",
    "# the row index, 'five' is not on the new series index ... \n",
    "# hence the corresponding value will be nan\n",
    "\n",
    "# there is no 'not-good' index in the data frame, ... \n",
    "# hence its values will not be aligned\n",
    "\n",
    "s = pd.Series(\n",
    "    data=['no', 'yes', 'yes', 'yes', 'bad'], \n",
    "    index=['two', 'one', 'four', 'three', 'not-good']\n",
    "    )\n",
    "\n",
    "# add a new column to the dataframe\n",
    "# note this can only be created using dictionary key indexing\n",
    "# as using attribute indexing will not work\n",
    "\n",
    "df['Visited'] = np.nan\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the series to the visited column\n",
    "\n",
    "# notice that the 'not-good' column doesn't match\n",
    "\n",
    "df.Visited = s\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---|||\n",
    "#### Deleting a column from a dataframe\n",
    "\n",
    "Using the **del** keyword followed by the column selection from the dataframe will delete the column from the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df['Visited']\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---||| \n",
    "\n",
    "#### Swapping Columns and Rows with transpose\n",
    "\n",
    "Using the **transpose function** or **T attribute** will:\n",
    "\n",
    "- swap rows to columns\n",
    "- columns to rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---|||\n",
    "\n",
    "### Creating a new Dataframe from an existing Dataframe\n",
    "\n",
    "By using an existing dataframe object, one can create a new dataframe.\n",
    "\n",
    "IF the index key is also specified, then\n",
    "\n",
    "- any index of the previous dataframe added to this new index, will have its column data in the new data frame\n",
    "\n",
    "- new index will automatically be assigned nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we only want the data in 'one, five and three' in the new dataframe\n",
    "\n",
    "# in the new index added, 'ten, nine' will be assigned nan \n",
    "\n",
    "df2 = pd.DataFrame(df, index=['one', 'ten', 'five', 'nine', 'three'])\n",
    "\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the name attributes of the index and column of a dataframe\n",
    "\n",
    "df2.columns.name = 'States in Nigeria Info'\n",
    "df2.index.name = \"Numbering\"\n",
    "\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return the values in a dataframe\n",
    "# the data are returned along the columns axis\n",
    "\n",
    "df.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---|||\n",
    "\n",
    "### Index Object\n",
    "\n",
    "This is a pandas object that holds the values of\n",
    "- a Series row index \n",
    "- a Dataframe's columns or row index A\n",
    "\n",
    "The Index Obeject is;\n",
    "\n",
    "- It is immutable \n",
    "- it can be shared among other data structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manipulatiing a Series using its index object\n",
    "\n",
    "s = pd.Series(\n",
    "    data=['Mo', 'Usman', 'Kolawole'],\n",
    "    index=['a', 'b', 'c']\n",
    "    )\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the index object\n",
    "\n",
    "ind = s.index\n",
    "\n",
    "ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index object are immutable\n",
    "\n",
    "ind[0] = 'k'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  create an index object\n",
    "\n",
    "ind = pd.Index(data=['l', 'm', 'n', 'p'])\n",
    "\n",
    "ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the new index object in a series\n",
    "\n",
    "s = pd.Series(np.arange(4), index=ind)\n",
    "\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind2 = ['l', 'm', 'n', 'p']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remember the difference between (==) and (is)?\n",
    "\n",
    "# compares element wise; remember vectorization?\n",
    "s.index == ind2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that they are the same object in memory \n",
    "\n",
    "s.index is ind2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.index is ind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---|||\n",
    "### Index Object as a container for Duplicate Object\n",
    "\n",
    "Since Index Object are immutable, they are similar to a fixed-set in Python, and support Set logic. But unlike Python Sets, they can contain duplicate values   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create two pandas object\n",
    "\n",
    "ind1 = pd.Index(['a', 'b', 'c', 'd', 'm', 'f', 'g', 'l'],)\n",
    "ind2 = pd.Index(['l', 'm', 'b', 'p', 'a', 'h', 'q', 'r'],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply set logic to both\n",
    "\n",
    "# concatenate two Index objects to create a new one\n",
    "\n",
    "ind3 = ind2.append(ind1)\n",
    "\n",
    "ind3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the difference between two set A - B\n",
    "# this is remove all the elements of B that is also in A,  from A\n",
    "\n",
    "ind4 = ind3.difference(ind2)\n",
    "\n",
    "ind4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the union of two or more\n",
    "\n",
    "# remember this is set logic, so duplicates will not be allowed\n",
    "# but an index object itself can contain duplicate values\n",
    "\n",
    "ind5 = ind1.union(ind2)\n",
    "\n",
    "ind5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---|||\n",
    "### Re-indexing a Series \n",
    "\n",
    "The **reindex** method allows creating a new Series object, with the data of the old series **aligned** to a new index (if the index of the data in the old series are elements in the new index).\n",
    "\n",
    "> the data in the new index will be ordered based on how they are passed in the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a series\n",
    "\n",
    "s = pd.Series(\n",
    "    data=np.arange(8),\n",
    "    index=['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h']\n",
    "    )\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index\n",
    "\n",
    "s.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the index using the reindex method\n",
    "\n",
    "# note there are new index values present, \n",
    "# so, index in **s** will be aligned to its data in the new series ...\n",
    "# and new index values will be assigned NAN values\n",
    "\n",
    "s.reindex(['a', 'i', 'j', 'k', 'b', 'l', 'm', 'c', 'n', 'p', 'd', 'q'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---|||\n",
    "### Filling the NAN values in the **reindex** method\n",
    "\n",
    "When both old and new index values are passed as elements in the reindex method,\n",
    "the new index values will by default be **aligned** to NAN values. To fill this NAN values, one can pass the method of filling the NAN values.\n",
    "\n",
    "**Methods**\n",
    "\n",
    "- ffill: fill the NAN values with the last valid data in the old series,\n",
    "- bfill: fill the next NAN with the  data,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the same series, from above\n",
    "\n",
    "s.reindex(\n",
    "    index=['a', 'i' , 'b', 'j', 'c', 'k', 'd', 'e'],\n",
    "    method='ffill'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.reindex?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---|||\n",
    "### NumPy UFunc and Mappings \n",
    "\n",
    "Remember UFuncs are numpy functions that apply an operation to each element of an ndarray, through broadcasting.\n",
    "\n",
    "In pandas, ufuncs can also be applied to pandas dataframe and series objects\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.arange(24).reshape((6 , 4)))\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply the sqrt ufunc of the table\n",
    "\n",
    "np.sqrt(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---|||\n",
    "### Apply Mapping Method\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pd.Series(np.arange(1, 10))\n",
    "df = pd.DataFrame(np.arange(36).reshape(9, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f =  lambda x: x*x\n",
    "\n",
    "def g(x):\n",
    "    return pd.Series([x.min(), x.max()], index=['min', 'max'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.apply(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.apply(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.apply(g, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---|||\n",
    "### **applymap** method\n",
    "\n",
    "This is similar to apply, except that it applies the passed in function to each element in the dataframe, instead of applying it to a a series along a  specific axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.random.random((5, 6))\n",
    "df = pd.DataFrame(data, index=['a', 'b', 'c', 'd', 'e'])\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "format = lambda x: '%.3f' %x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using apply will try to execute format on the axis (0, by default),\n",
    "# which should fail, because the format method except single values,\n",
    "# but the apply method will pass a series object to it\n",
    "\n",
    "df.apply(format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to overcome this challenge, the applymap method is used\n",
    "\n",
    "df.applymap(format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---|||\n",
    "### Sorting\n",
    "\n",
    "Sorting can be done in two ways:\n",
    "\n",
    "- **by values**: sort based on the value in the series or dataframe\n",
    "- **by index**: sort based on the index in the series OR based on the axis in a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pd.Series(np.arange(1, 14, 2), index=['c', 'a', 'b', 'k', 'i', 'm', 'e'])\n",
    "\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort by index in descending order\n",
    "\n",
    "s = s.sort_index(ascending=False)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort by values in ascending\n",
    "\n",
    "s.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.arange(1, 41, 3).reshape(7, 2), index=[3, 1, 7, 11, 32, 9, 0])\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort by index along axis = 1\n",
    "\n",
    "df.sort_index(axis=1, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort by index along axis=0\n",
    "\n",
    "df.sort_index(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort by values along axis = 1, but a key must be specified\n",
    "\n",
    "df.sort_values(axis=1, by=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---|||\n",
    "# Chapter 6: Data Loading, Storage, and File Formats\n",
    "\n",
    "---|||\n",
    "### Loading/Reading of Data into a Dataframe Object\n",
    "\n",
    "The read_[type] is meant to convert data stored on disk to a Dataframe object.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading a csv formatted data\n",
    "\n",
    "file_path = \"../datasets/Employees.csv\"\n",
    "\n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the index column to Unnamed\n",
    "df = df.set_index('Unnamed: 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove index name\n",
    "df.index.name = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this multiple operations could be done directly when loading the file\n",
    "\n",
    "df = pd.read_csv(file_path, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---|||\n",
    "# Using **read_table** as a general method for loading text data\n",
    "\n",
    "While *read_csv* is specific to CSV formatted files, one can use *read_table*, and indicate how the data is formatted in an optional seperator key (sep).\n",
    "\n",
    "The seperator between data points for CSV-formatted file is a comma (,)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use read_tables to open the Employee CSV data\n",
    "\n",
    "df = pd.read_table(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the format method in the seperator\n",
    "# remember the index column\n",
    "\n",
    "df = pd.read_table(file_path, sep=',', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"../datasets/pydata_datasets/babynames/yob1881.txt\"\n",
    "\n",
    "# suppose we don't know how the data is formatted, we use the read_table to peek\n",
    "\n",
    "df = pd.read_table(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since it is comma seperated, we use the read_csv file\n",
    "\n",
    "df = pd.read_csv(file_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---|||\n",
    "### Using the header key\n",
    "\n",
    "\n",
    "It is used to specify whether the data has column names(header) associated with it. Whenever the header argument is still specified, then;\n",
    "\n",
    "- it value is either 0 or NONE\n",
    "- the default column names (0, 1, 2, 3, ...) based on the inferenced number of columns \n",
    "\n",
    "When the **names** key is specified, header must be specified with...\n",
    "\n",
    "- None: if column names are not present\n",
    "- 0: if they are present, but will be renamed (using names=[val1, val2, ...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# something is amiss, the column names are part of the data\n",
    "# so we specify that the header (=> column names) is/are absent\n",
    "\n",
    "df = pd.read_csv(file_path, header=None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---|||\n",
    "### Setting the column names\n",
    "\n",
    "This can be achieved by passing a values to the **names** optional parameter in the read_csv method.\n",
    "\n",
    "There are implications if the length of the passed values exceed those in the data file. In this case, the name is added and populated with NAN values\n",
    "\n",
    "If the length of the passed values were less than those in the data file, then the the columns not accounted for are used to index the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to be even more pragmatic, by specify the names we want\n",
    "# instead of using the default integer column indexing\n",
    "\n",
    "df = pd.read_csv(file_path, header=None, names=['Name', 'Sex', 'Count'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"../datasets/pydata_datasets/haiti/Haiti.csv\"\n",
    "\n",
    "# peek into the data\n",
    "\n",
    "df = pd.read_csv(file_path)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the count of each columns of the dataframe\n",
    "\n",
    "df.count() \n",
    "\n",
    "# observe that the total length is 3593"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm the observation using the shape\n",
    "\n",
    "df.shape\n",
    "\n",
    "# its true; there are 3593 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# is the serial number a numbering format or special?\n",
    "\n",
    "# sort by index\n",
    "df.sort_values(by='Serial').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the values of serial to start from 1, by subtracting 3\n",
    "\n",
    "# since we have a dataframe, and we want to apply and operation on...\n",
    "# each values in the Serial column, we use the map method\n",
    "\n",
    "f = lambda x: x - 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Serial = df.loc[:, 'Serial'] - 3  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reindex by Serial\n",
    "\n",
    "df.set_index('Serial', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# an interesting data set\n",
    "file_path = \"../datasets/pydata_datasets/movielens/movies.dat\"\n",
    "\n",
    "# peek\n",
    "pd.read_table(file_path).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observations:\n",
    "\n",
    "- the file has no header: set header to [name, year, genre]\n",
    "- we have three seperator :: to get the title and genre, () to get the year\n",
    "- we should replace the |, with something better , a space\n",
    "\n",
    "- the index of the data should be the first column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_table(\n",
    "    file_path,\n",
    "    header=None,\n",
    "    names=['num', \"title\", \"year\", \"genre\", ''],\n",
    "    sep='(\\d+)::([^:]+)\\s\\((\\d+)\\)::(.*)',\n",
    "    engine='python',\n",
    "    keep_default_na=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---|||\n",
    "## Handling Missing Values \n",
    "\n",
    "It is possible to pass a string or sequence of strings, as values to the **na_values** key, which would marks any occurence of the string(s) as missing values (NAN, NA...) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets set any Abel and Echo as sentinel value\n",
    "\n",
    "df = pd.read_csv(\"../datasets/Employees.csv\", na_values=['Able', 'Echo'])\n",
    "\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-index\n",
    "\n",
    "df.set_index('Unnamed: 0', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename index to empty \n",
    "df.rename_axis(index=\"\", inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---|||\n",
    "### **na_values** for all or specific column(s)\n",
    "\n",
    "It is possible to specify that when a given data, as specified in the **value(s)** passed to the **na_values** key, then the data would be marked as sentinel i.e. the data would be marked as missing if any is found in the dataframe.\n",
    "\n",
    "It is also possible to specify that we want the matches in specific columns by passing a dictionary, containing a **key-value** pair to **na_values**, such that;\n",
    "\n",
    "- the key is the specific column name where we want to mark a certain data as sentinel\n",
    "\n",
    "- the value(s) is/are the data to mark as sentinel in the specified column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we could have set the sentinel by selecting the specific column\n",
    "\n",
    "# lets set any Abel and Echo as sentinel value in the Name column\n",
    "#and all the values less than four (4) in the YearOfService column\n",
    "\n",
    "df = pd.read_csv(\n",
    "    \"../datasets/Employees.csv\", \n",
    "    na_values={\n",
    "        'Name': ['Able', 'Echo'],\n",
    "        'YearOfService': [0, 1, 2, 3]       # values less than 4\n",
    "        },\n",
    "\n",
    "    # when names is specified, header must be specified with...\n",
    "    #   None: if column names are not present\n",
    "    #   0: if they are present, but will be renamed (using names=[])\n",
    "    header=0, # this allows us to specify no column\n",
    "    names= [\"\", \"Department\", \"Name\", \"YearOfService\"], # rename column\n",
    "    index_col=0\n",
    ")\n",
    "\n",
    "df.sort_values(by=['Name'], ascending=True).tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---|||\n",
    "### Converter Optional Parameter\n",
    "\n",
    "This is a **read_csv, and read_table** optional parameter, that take a dictionary as value.\n",
    "\n",
    "Its purpose is to apply a **function/mapping f** specified as value to a **key representing the column name, that the function should apply the mapping** to every values in the specified column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_table(\n",
    "    \"../datasets/a.txt\",\n",
    "    header=0,\n",
    "    names=[\"Deparment\", \"Name\", \"YearOfService\"],\n",
    "    sep=','\n",
    "    # index_col=0\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on the observation above, lets change the NAN values in Name to 'MO'abs\n",
    "\n",
    "f = lambda x: ('Mo' if not x else x )\n",
    "# def f(x):\n",
    "    # if x\n",
    "df = pd.read_table(\n",
    "    \"../datasets/a.txt\",\n",
    "    header=0,\n",
    "    names=[\"Deparment\", \"Name\", \"YearOfService\"],\n",
    "    sep=',',\n",
    "    converters={'Name': f}\n",
    "    # index_col=0\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---|||\n",
    "### Reading Text Files in Pieces\n",
    "\n",
    "It is optimal, when reading data in large files, to read the data from the file in small pieces OR iterate through smaller chunks of the file\n",
    "\n",
    "One way this can be achieved is to specify the number of rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the display to more compact format\n",
    "\n",
    "pd.options.display.max_rows = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"../datasets/pydata_datasets/movielens/movies.dat\"\n",
    "\n",
    "# take a look at the resulting dataframe for example\n",
    "df = pd.read_table(\n",
    "    file_path,\n",
    "    header=None,\n",
    "    names=['num', \"title\", \"year\", \"genre\", ''],\n",
    "    sep='(\\d+)::([^:]+)\\s\\((\\d+)\\)::(.*)',\n",
    "    engine='python',\n",
    "    keep_default_na=False,\n",
    ")\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 3883 rows.\n",
    "\n",
    "Suppose it was very large, then it would take a lot longer to parse the entire data into the dataframe. One way to avoid this is to specify the **number of rows** we want from the file.\n",
    "\n",
    "By passing the **nrows** keys, the read_[format] will stop when it reaches the number of rows value specified. This is especially useful when one wants to quickly examine data in a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instead of reading the entire file, we could specify the number of rows\n",
    "\n",
    "file_path = \"../datasets/pydata_datasets/movielens/movies.dat\"\n",
    "\n",
    "df = pd.read_table(\n",
    "    file_path,\n",
    "    header=None,\n",
    "    names=['num', \"title\", \"year\", \"genre\", ''],\n",
    "    sep='(\\d+)::([^:]+)\\s\\((\\d+)\\)::(.*)',\n",
    "    engine='python',\n",
    "    keep_default_na=False,\n",
    "    nrows=3\n",
    ")\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---|||\n",
    "### Reading in chunks, by specifying the chunksize\n",
    "\n",
    "When the chunksize value is specified, an iterator over the chunks of data in the file is returned.\n",
    "\n",
    "Here, if the **chunksize == p**, then p amount of rows will be returned everytime we call **next** on the iterator (or we iterate using for loop), until, there are no more data to iterate over\n",
    "\n",
    "More interestingly, we can call the **get_chunks(size=val)** method, and specify a certain **val**, over the iterator returned, to\n",
    "\n",
    "- read less than the value of rows that would have been returned in a given iteration,  **if val < p**\n",
    "\n",
    "- read more than the value of rows that would have been returned in a given iteration, **if val > p**\n",
    "\n",
    "- read a given number of rows from a given iteration, based on the value of the specified size in the **get_chunks** method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"../datasets/pydata_datasets/movielens/movies.dat\"\n",
    "\n",
    "# specify the chunksize to get an iterator\n",
    "\n",
    "chunk = pd.read_table(\n",
    "    file_path,\n",
    "    header=None,\n",
    "    names=['num', \"title\", \"year\", \"genre\", ''],\n",
    "    sep='(\\d+)::([^:]+)\\s\\((\\d+)\\)::(.*)',\n",
    "    engine='python',\n",
    "    keep_default_na=False,\n",
    "    chunksize=20\n",
    ")\n",
    "\n",
    "# in this iteration get the firs 20 rows\n",
    "next(chunk)\n",
    "\n",
    "# in this iteration, we get 22 rows; more than the chunksize specified\n",
    "print(chunk.get_chunk(size=22).shape)\n",
    "\n",
    "# in this iteration, we get the default chunksize specfied; 20\n",
    "print((next(chunk).shape))\n",
    "\n",
    "# in this iteration, we get 10 rows; less than the chunksize specified\n",
    "print(chunk.get_chunk(size=10).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"../datasets/pydata_datasets/movielens/movies.dat\"\n",
    "\n",
    "# specify the chunksize to get an iterator\n",
    "\n",
    "chunk = pd.read_table(\n",
    "    file_path,\n",
    "    header=None,\n",
    "    names=['num', \"title\", \"year\", \"genre\", ''],\n",
    "    sep='(\\d+)::([^:]+)\\s\\((\\d+)\\)::(.*)',\n",
    "    engine='python',\n",
    "    keep_default_na=False,\n",
    "    chunksize=10\n",
    ")\n",
    "\n",
    "# print the number of iterations it takes to read the entire file\n",
    "count = 0\n",
    "tot_rows = 0\n",
    "for piece  in chunk:\n",
    "    count += 1\n",
    "    tot_rows += piece.shape[0]\n",
    "    # print((piece.shape))\n",
    "\n",
    "'It took {0} iterations to read {1} number of rows'.format(count,  tot_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---|||\n",
    "### Writing Data To Text Format\n",
    "\n",
    "Using the **to_[format]**, we can save a dataframe data to disk based on the format specified\n",
    "\n",
    "\n",
    "dataframe.to_[format]([name.[format])\n",
    "\n",
    "\n",
    "- dataframe.to_excel([name.xlsl]): to excel format\n",
    "- dataframe.to_csv([name.csv]): to csv formatted\n",
    "- dataframe.to_json([name.json]): to json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"../datasets/Employees.csv\"\n",
    "\n",
    "df = pd.read_csv(\"../datasets/Employees.csv\", index_col=0)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# to json; the record format omit the index \n",
    "\n",
    "df.to_json(\"employee.json\", \"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to csv fomat, \n",
    "# but instead of the delimeter being comma, we use |\n",
    "\n",
    "df.to_csv(\"employee.csv\", sep=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# observe the following data\n",
    "file_path = \"a.txt\"\n",
    "\n",
    "# since it is a .txt file, we use the read_table format\n",
    "# actually, we could have used a read_csv, since we know it is comma seperated\n",
    "\n",
    "df = pd.read_table(file_path, sep=',', index_col=0)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observations\n",
    "\n",
    "There are missing values in the Name column, to replace this, missing values with another value, we could have used the **converter** method while reading the file.\n",
    "\n",
    "However, we want to save this data to disk, and replace any missing values with **another name**.\n",
    "\n",
    "To achieve the above, we pass the **value** to **na_rep** key. This value will  stand in for the missing values in the dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"employee_NULL_for_NA.csv\", sep='|', na_rep=\"NULL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we could also remove the column labels by passing a header as false\n",
    "df.to_csv(\"employee_NULL_for_NA_w-o_header.csv\", sep='|', na_rep=\"NULL\", header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we could also remove the index by passing an index key as false\n",
    "df.to_csv(\"employee_NULL_for_NA_w-o_header_w-o_index.csv\", sep='|', na_rep=\"NULL\", header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instead of writing the output to a file, by passing the file name,\n",
    "# we could output the result into standard output, which will just print the result\n",
    "\n",
    "# get the stdout\n",
    "from sys import stdout\n",
    "\n",
    "df.to_csv(stdout, sep='|', na_rep=\"NULL\", header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can also indicate which columns we are interested in\n",
    "\n",
    "# we could also remove the index by passing an index key as false\n",
    "df.to_csv(stdout, sep='|', na_rep=\"NULL\", index=False, columns=[\"Department\", \"YearOfService\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---|||\n",
    "### Binary Data Formats\n",
    "\n",
    "One can take data in a different format and store it in binary format, in a process known as serialization\n",
    "\n",
    "The reverse is known as deserialization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read a csv file and save it in binary format\n",
    "\n",
    "f = lambda x: \"NULL\" if not x else x\n",
    "\n",
    "df = pd.read_table(\"a.txt\", index_col=['Unnamed: 0'], sep=',', converters={'Name': f})\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store it to binary format\n",
    "\n",
    "df_bin = df.to_pickle(\"employee_NULL_w-o_header_to_binary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the binary file\n",
    "\n",
    "df = pd.read_pickle(\"employee_NULL_w-o_header_to_binary\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---|||\n",
    "### Storage Formats\n",
    "\n",
    "HDF5: Hierachical Data Format\n",
    "\n",
    "This is a file format used for **storing large quantities** of scientific array data. \n",
    "\n",
    "An HDF5 file can store multiple datasets, and metadata as a key-value pair. Interestingly, it supports the compression of those file, using **a variety of compression modes**, so that data with repeated pattern are stored efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store a dataframe in HDF5 format\n",
    "\n",
    "# create the dataframe\n",
    "df = pd.DataFrame({'a': np.random.randn(100)})\n",
    "\n",
    "# store as hdf\n",
    "store = pd.HDFStore('data.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the dataframe\n",
    "\n",
    "store['obj1'] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "store['obj1_col'] = df['a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.io.pytables.HDFStore'>\n",
      "File path: data.h5\n",
      "/obj1                frame        (shape->[100,1])\n",
      "/obj1_col            series       (shape->[100])  \n"
     ]
    }
   ],
   "source": [
    "print(store.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.245699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.326683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.405154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.338620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.736357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>-0.116920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>-0.420591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.437353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>-0.391695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0.990099</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows  1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           a\n",
       "0  -1.245699\n",
       "1   0.326683\n",
       "2  -1.405154\n",
       "3  -0.338620\n",
       "4   0.736357\n",
       "..       ...\n",
       "95 -0.116920\n",
       "96 -0.420591\n",
       "97  0.437353\n",
       "98 -0.391695\n",
       "99  0.990099\n",
       "\n",
       "[100 rows x 1 columns]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# retrieve the df\n",
    "\n",
    "store['obj1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    -1.245699\n",
       "1     0.326683\n",
       "2    -1.405154\n",
       "3    -0.338620\n",
       "4     0.736357\n",
       "        ...   \n",
       "95   -0.116920\n",
       "96   -0.420591\n",
       "97    0.437353\n",
       "98   -0.391695\n",
       "99    0.990099\n",
       "Name: a, Length: 100, dtype: float64"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# retreive the data in col a\n",
    "\n",
    "store['obj1_col']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---|||\n",
    "### Storage Schema\n",
    "\n",
    "Two schema (i.e. mode of storing data) are supported by HDF5Store;\n",
    "\n",
    "- fixed: fast, but doen't support query operations\n",
    "\n",
    "- table: slow, but supports query operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame({'b': np.random.randn(40), 'e': np.random.randn(40)})\n",
    "\n",
    "store.put('obj2', value=df2, format='table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>b</th>\n",
       "      <th>e</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.021936</td>\n",
       "      <td>-0.729077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.738394</td>\n",
       "      <td>1.231876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.927187</td>\n",
       "      <td>-1.120192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-0.121029</td>\n",
       "      <td>1.007038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.172373</td>\n",
       "      <td>0.689595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.593094</td>\n",
       "      <td>0.022941</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           b         e\n",
       "10  0.021936 -0.729077\n",
       "11 -0.738394  1.231876\n",
       "12  0.927187 -1.120192\n",
       "13 -0.121029  1.007038\n",
       "14  1.172373  0.689595\n",
       "15  0.593094  0.022941"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# note, if storage schema is \"fixed, then this query operation will fail\n",
    "store.select('obj2', where=['index >= 10 and index <= 15'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# close the storage\n",
    "\n",
    "store.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "ename": "ClosedFileError",
     "evalue": "data.h5 file is not open!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mClosedFileError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[108], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# confirm close\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mstore\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mobj1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/pytables.py:595\u001b[0m, in \u001b[0;36mHDFStore.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key: \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 595\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/pytables.py:794\u001b[0m, in \u001b[0;36mHDFStore.get\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    779\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    780\u001b[0m \u001b[38;5;124;03mRetrieve pandas object stored in file.\u001b[39;00m\n\u001b[1;32m    781\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    789\u001b[0m \u001b[38;5;124;03m    Same type as object stored in file.\u001b[39;00m\n\u001b[1;32m    790\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    791\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m patch_pickle():\n\u001b[1;32m    792\u001b[0m     \u001b[38;5;66;03m# GH#31167 Without this patch, pickle doesn't know how to unpickle\u001b[39;00m\n\u001b[1;32m    793\u001b[0m     \u001b[38;5;66;03m#  old DateOffset objects now that they are cdef classes.\u001b[39;00m\n\u001b[0;32m--> 794\u001b[0m     group \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    795\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m group \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    796\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo object named \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in the file\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/pytables.py:1500\u001b[0m, in \u001b[0;36mHDFStore.get_node\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_node\u001b[39m(\u001b[38;5;28mself\u001b[39m, key: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Node \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1499\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"return the node with the key or None if it does not exist\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1500\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_if_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1501\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m key\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1502\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m key\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/io/pytables.py:1629\u001b[0m, in \u001b[0;36mHDFStore._check_if_open\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1627\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_if_open\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m   1628\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_open:\n\u001b[0;32m-> 1629\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ClosedFileError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m file is not open!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mClosedFileError\u001b[0m: data.h5 file is not open!"
     ]
    }
   ],
   "source": [
    "# confirm close\n",
    "\n",
    "store['obj1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# performing the same operation using read_hdf5, and to_hddf. \n",
    "a = np.arange(20).reshape((10, 2))\n",
    "\n",
    "df  = pd.DataFrame(a, columns=['a', 'b'])\n",
    "\n",
    "df.to_hdf('rdata.h5', 'obj', format='table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: [3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_hdf('rdata.h5', 'obj', where= ['index >=3', 'columns = a' ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
